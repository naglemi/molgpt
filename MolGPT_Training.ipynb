{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤  MolGPT Training - Cowboy Chronicle ðŸ¤ \n",
    "\n",
    "\n",
    "\n",
    "This notebook demonstrates how to train the MolGPT model on molecular datasets. MolGPT is a transformer-decoder model for molecular generation that can be trained on SMILES strings with or without conditional properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "\n",
    "\n",
    "First, let's make sure we have all the necessary imports and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Import directly from the files instead of using package imports\n",
    "sys.path.insert(0, '.')\n",
    "from train.model import GPT, GPTConfig\n",
    "from train.trainer import Trainer, TrainerConfig\n",
    "from train.dataset import SmileDataset\n",
    "from train.utils import set_seed\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "\n",
    "\n",
    "Let's load the dataset and explore its structure. We'll use the Moses dataset for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Moses dataset\n",
    "data_name = 'moses2'\n",
    "data = pd.read_csv(f'datasets/{data_name}.csv')\n",
    "data = data.dropna(axis=0).reset_index(drop=True)\n",
    "data.columns = data.columns.str.lower()\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets\n",
    "train_data = data[data['split'] == 'train'].reset_index(drop=True)\n",
    "val_data = data[data['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Training\n",
    "\n",
    "\n",
    "\n",
    "Now we'll prepare the data for training by tokenizing the SMILES strings and creating the dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SMILES and scaffold strings\n",
    "smiles = train_data['smiles']\n",
    "vsmiles = val_data['smiles']\n",
    "scaffold = train_data['scaffold_smiles']\n",
    "vscaffold = val_data['scaffold_smiles']\n",
    "\n",
    "# Define the regex pattern for tokenizing SMILES\n",
    "pattern = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "regex = re.compile(pattern)\n",
    "\n",
    "# Calculate maximum lengths\n",
    "lens = [len(regex.findall(i.strip())) for i in (list(smiles.values) + list(vsmiles.values))]\n",
    "max_len = max(lens)\n",
    "print(f'Max SMILES length: {max_len}')\n",
    "\n",
    "lens = [len(regex.findall(i.strip())) for i in (list(scaffold.values) + list(vscaffold.values))]\n",
    "scaffold_max_len = max(lens)\n",
    "print(f'Max scaffold length: {scaffold_max_len}')\n",
    "\n",
    "# Pad the SMILES strings\n",
    "smiles = [i + str('<')*(max_len - len(regex.findall(i.strip()))) for i in smiles]\n",
    "vsmiles = [i + str('<')*(max_len - len(regex.findall(i.strip()))) for i in vsmiles]\n",
    "\n",
    "scaffold = [i + str('<')*(scaffold_max_len - len(regex.findall(i.strip()))) for i in scaffold]\n",
    "vscaffold = [i + str('<')*(scaffold_max_len - len(regex.findall(i.strip()))) for i in vscaffold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the character set\n",
    "whole_string = ['#', '%10', '%11', '%12', '(', ')', '-', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH2-]', '[BH3-]', '[B]', '[C+]', '[C-]', '[CH+]', '[CH-]', '[CH2+]', '[CH2]', '[CH]', '[F+]', '[H]', '[I+]', '[IH2]', '[IH]', '[N+]', '[N-]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[N]', '[O+]', '[O-]', '[OH+]', '[O]', '[P+]', '[PH+]', '[PH2+]', '[PH]', '[S+]', '[S-]', '[SH+]', '[SH]', '[Se+]', '[SeH+]', '[SeH]', '[Se]', '[Si-]', '[SiH-]', '[SiH2]', '[SiH]', '[Si]', '[b-]', '[bH-]', '[c+]', '[c-]', '[cH+]', '[cH-]', '[n+]', '[n-]', '[nH+]', '[nH]', '[o+]', '[s+]', '[sH+]', '[se+]', '[se]', 'b', 'c', 'n', 'o', 'p', 's']\n",
    "\n",
    "# Extract property values for conditional training\n",
    "props = ['logp']\n",
    "prop = train_data[props].values.tolist()\n",
    "vprop = val_data[props].values.tolist()\n",
    "num_props = len(props)\n",
    "\n",
    "# Create dataset objects\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.debug = False\n",
    "        self.scaffold = True\n",
    "        self.lstm = False\n",
    "        self.lstm_layers = 0\n",
    "        self.num_props = num_props\n",
    "        self.props = props\n",
    "\n",
    "args = Args()\n",
    "\n",
    "train_dataset = SmileDataset(args, smiles, whole_string, max_len, prop=prop, aug_prob=0, scaffold=scaffold, scaffold_maxlen=scaffold_max_len)\n",
    "valid_dataset = SmileDataset(args, vsmiles, whole_string, max_len, prop=vprop, aug_prob=0, scaffold=vscaffold, scaffold_maxlen=scaffold_max_len)\n",
    "\n",
    "print(f\"Vocabulary size: {train_dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model Architecture\n",
    "\n",
    "\n",
    "\n",
    "Now we'll define the model architecture using the GPT configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "n_embd = 256\n",
    "\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.max_len, \n",
    "                  num_props=num_props,\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=n_embd, \n",
    "                  scaffold=args.scaffold, scaffold_maxlen=scaffold_max_len,\n",
    "                  lstm=args.lstm, lstm_layers=args.lstm_layers)\n",
    "\n",
    "# Create the model\n",
    "model = GPT(mconf)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "\n",
    "\n",
    "Now we'll set up the training configuration and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training configuration\n",
    "batch_size = 32  # Reduced for demonstration\n",
    "max_epochs = 2   # Reduced for demonstration\n",
    "learning_rate = 6e-4\n",
    "run_name = 'logp_moses_demo'\n",
    "\n",
    "tconf = TrainerConfig(\n",
    "    max_epochs=max_epochs, \n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate,\n",
    "    lr_decay=True, \n",
    "    warmup_tokens=0.1*len(train_data)*max_len, \n",
    "    final_tokens=max_epochs*len(train_data)*max_len,\n",
    "    num_workers=2, \n",
    "    ckpt_path=f'weights/{run_name}.pt', \n",
    "    block_size=train_dataset.max_len, \n",
    "    generate=False\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(model, train_dataset, valid_dataset, tconf, train_dataset.stoi, train_dataset.itos)\n",
    "\n",
    "# For demonstration, we'll skip the actual training since it would take too long\n",
    "# In a real scenario, you would run:\n",
    "# df = trainer.train(None)  # Set to None since we're not using wandb\n",
    "\n",
    "print(\"Training would start here in a real scenario.\")\n",
    "print(\"For demonstration purposes, we'll use pre-trained weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Results\n",
    "\n",
    "\n",
    "\n",
    "Let's create some mock training results to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock training results\n",
    "epochs = list(range(1, 11))\n",
    "train_loss = [2.5, 2.2, 1.9, 1.7, 1.5, 1.4, 1.3, 1.25, 1.2, 1.18]\n",
    "val_loss = [2.6, 2.3, 2.0, 1.8, 1.65, 1.55, 1.5, 1.45, 1.4, 1.38]\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model Weights\n",
    "\n",
    "\n",
    "\n",
    "In a real training scenario, the model weights would be saved automatically by the trainer. For demonstration, we'll show how you would save the weights manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights (commented out since we're not actually training)\n",
    "# torch.save(model.state_dict(), f'weights/{run_name}.pt')\n",
    "print(f\"Model weights would be saved to: weights/{run_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "\n",
    "\n",
    "1. Set up the environment for MolGPT training\n",
    "\n",
    "2. Load and explore the molecular dataset\n",
    "\n",
    "3. Prepare the data for training\n",
    "\n",
    "4. Define the model architecture\n",
    "\n",
    "5. Configure and (simulate) training the model\n",
    "\n",
    "6. Visualize training results\n",
    "\n",
    "7. Save model weights\n",
    "\n",
    "\n",
    "\n",
    "The MolGPT model can be trained with different conditioning options:\n",
    "\n",
    "- Unconditional generation\n",
    "\n",
    "- Property-based conditional generation (e.g., logP, QED, SAS)\n",
    "\n",
    "- Scaffold-based conditional generation\n",
    "\n",
    "- Combined property and scaffold-based conditional generation\n",
    "\n",
    "\n",
    "\n",
    "For actual training, you would need to run the training process for more epochs and with larger batch sizes, which would take several hours depending on your hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (molgpt)",
   "language": "python",
   "name": "molgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}