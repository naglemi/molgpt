{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§† MolGPT Cowboy Chronicle ü§†\n",
    "\n",
    "This notebook demonstrates the complete workflow for MolGPT - a GPT-based model for molecular generation. We'll cover:\n",
    "\n",
    "1. **Setup & Environment** - Setting up dependencies and data\n",
    "2. **Training** - Training the MolGPT model on molecular data\n",
    "3. **Generation** - Generating new molecules with various conditioning strategies\n",
    "4. **Evaluation** - Evaluating the quality of generated molecules\n",
    "\n",
    "Let's saddle up and ride! üêé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment\n",
    "\n",
    "First, let's make sure we have all the necessary imports and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import Crippen\n",
    "from rdkit.Chem.Descriptors import ExactMolWt\n",
    "from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
    "\n",
    "# Import directly from the files instead of using package imports\n",
    "sys.path.insert(0, '.')\n",
    "from train.model import GPT, GPTConfig\n",
    "from train.trainer import Trainer, TrainerConfig\n",
    "from train.dataset import SmileDataset\n",
    "from train.utils import set_seed\n",
    "from generate.utils import sample, check_novelty, canonic_smiles\n",
    "from moses.utils import get_mol\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Dataset\n",
    "\n",
    "Let's load the Moses dataset and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Moses dataset\n",
    "data_name = 'moses2'\n",
    "data = pd.read_csv(f'{data_name}.csv')\n",
    "data = data.dropna(axis=0).reset_index(drop=True)\n",
    "data.columns = data.columns.str.lower()\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenization\n",
    "\n",
    "We need to tokenize the SMILES strings for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex pattern for tokenizing SMILES\n",
    "pattern = \"(\\[[^\\]]+]|<|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "regex = re.compile(pattern)\n",
    "\n",
    "# Define the character set\n",
    "whole_string = ['#', '%10', '%11', '%12', '(', ')', '-', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<', '=', 'B', 'Br', 'C', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', '[B-]', '[BH-]', '[BH2-]', '[BH3-]', '[B]', '[C+]', '[C-]', '[CH+]', '[CH-]', '[CH2+]', '[CH2]', '[CH]', '[F+]', '[H]', '[I+]', '[IH2]', '[IH]', '[N+]', '[N-]', '[NH+]', '[NH-]', '[NH2+]', '[NH3+]', '[N]', '[O+]', '[O-]', '[OH+]', '[O]', '[P+]', '[PH+]', '[PH2+]', '[PH]', '[S+]', '[S-]', '[SH+]', '[SH]', '[Se+]', '[SeH+]', '[SeH]', '[Se]', '[Si-]', '[SiH-]', '[SiH2]', '[SiH]', '[Si]', '[b-]', '[bH-]', '[c+]', '[c-]', '[cH+]', '[cH-]', '[n+]', '[n-]', '[nH+]', '[nH]', '[o+]', '[s+]', '[sH+]', '[se+]', '[se]', 'b', 'c', 'n', 'o', 'p', 's']\n",
    "\n",
    "# Create vocabulary mappings\n",
    "stoi = {ch: i for i, ch in enumerate(whole_string)}\n",
    "itos = {i: ch for i, ch in enumerate(whole_string)}\n",
    "\n",
    "# Save vocabulary mappings to JSON files\n",
    "with open(f'{data_name}_stoi.json', 'w') as f:\n",
    "    json.dump(stoi, f)\n",
    "\n",
    "print(f\"Vocabulary size: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "\n",
    "Now let's train the MolGPT model on the Moses dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "vocab_size = len(stoi)\n",
    "block_size = 54  # Maximum SMILES length\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "n_embd = 256\n",
    "scaffold_max_len = 48  # For Moses dataset\n",
    "\n",
    "# Define model configuration\n",
    "mconf = GPTConfig(vocab_size, block_size, \n",
    "                  num_props=1,  # Using 1 property for conditioning\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=n_embd, \n",
    "                  scaffold=True, scaffold_maxlen=scaffold_max_len,\n",
    "                  lstm=False, lstm_layers=0)\n",
    "\n",
    "# Create the model\n",
    "model = GPT(mconf)\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for training\n",
    "train_data = data[data['split'] == 'train']\n",
    "val_data = data[data['split'] == 'test']\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "\n",
    "# Extract SMILES strings\n",
    "train_smiles = train_data['smiles'].values\n",
    "val_smiles = val_data['smiles'].values\n",
    "\n",
    "# Extract scaffolds\n",
    "train_scaffolds = train_data['scaffold_smiles'].values\n",
    "val_scaffolds = val_data['scaffold_smiles'].values\n",
    "\n",
    "# Extract properties (QED for this example)\n",
    "train_props = train_data['qed'].values\n",
    "val_props = val_data['qed'].values\n",
    "\n",
    "# Combine all SMILES for vocabulary creation\n",
    "all_smiles = np.concatenate([train_smiles, val_smiles])\n",
    "all_scaffolds = np.concatenate([train_scaffolds, val_scaffolds])\n",
    "content = ' '.join(all_smiles.tolist() + all_scaffolds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple argparse-like object for the dataset\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.debug = False\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SmileDataset(args, train_smiles, content, block_size, aug_prob=0.5, \n",
    "                            prop=train_props, scaffold=train_scaffolds, scaffold_maxlen=scaffold_max_len)\n",
    "val_dataset = SmileDataset(args, val_smiles, content, block_size, aug_prob=0.0, \n",
    "                          prop=val_props, scaffold=val_scaffolds, scaffold_maxlen=scaffold_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training\n",
    "train_config = TrainerConfig(\n",
    "    max_epochs=2,  # For demonstration, use a small number of epochs\n",
    "    batch_size=64,\n",
    "    learning_rate=3e-4,\n",
    "    lr_decay=True,\n",
    "    warmup_tokens=512*20,\n",
    "    final_tokens=2*len(train_smiles)*block_size,\n",
    "    ckpt_path='trained_model.pt',\n",
    "    num_workers=4,\n",
    "    generate=True\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(model, train_dataset, val_dataset, train_config, stoi, itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For actual training, uncomment the following line\n",
    "# df = trainer.train(wandb=None)\n",
    "\n",
    "print(\"Training skipped for demonstration purposes.\")\n",
    "print(\"In a real scenario, you would run the training process for more epochs and with larger batch sizes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generation\n",
    "\n",
    "Now let's use the pre-trained model to generate new molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "vocab_size = len(stoi)\n",
    "block_size = 54  # Maximum SMILES length\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "n_embd = 256\n",
    "scaffold_max_len = 48  # For Moses dataset\n",
    "\n",
    "# Choose the model type\n",
    "model_type = \"qed\"  # Options: qed, sas, logp, tpsa\n",
    "model_weight = f\"/home/ubuntu/molgpt/datasets/weights/moses_scaf_wholeseq_{model_type}.pt\"\n",
    "\n",
    "# Define model configuration\n",
    "mconf = GPTConfig(vocab_size, block_size, \n",
    "                  num_props=1,  # Using 1 property for conditioning\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=n_embd, \n",
    "                  scaffold=True, scaffold_maxlen=scaffold_max_len,\n",
    "                  lstm=False, lstm_layers=0)\n",
    "\n",
    "# Create the model\n",
    "model = GPT(mconf)\n",
    "\n",
    "# Load pre-trained weights\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_weight, map_location=device))\n",
    "    print(f\"Model loaded from {model_weight}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Using untrained model for demonstration purposes.\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set generation parameters\n",
    "context = \"C\"  # Starting with a carbon atom\n",
    "batch_size = 5  # Generate 5 molecules at once\n",
    "temperature = 1.0  # Temperature for sampling (higher = more diverse)\n",
    "top_k = None  # No top-k sampling\n",
    "\n",
    "# Tokenize the context\n",
    "x = torch.tensor([stoi[s] for s in regex.findall(context)], dtype=torch.long)[None,...].repeat(batch_size, 1).to(device)\n",
    "\n",
    "# Generate molecules\n",
    "with torch.no_grad():\n",
    "    y = sample(model, x, block_size, temperature=temperature, sample=True, top_k=top_k, prop=None, scaffold=None)\n",
    "\n",
    "# Convert generated tokens to SMILES strings\n",
    "generated_smiles = []\n",
    "for gen_mol in y:\n",
    "    completion = ''.join([itos[int(i)] for i in gen_mol])\n",
    "    completion = completion.replace('<', '')  # Remove padding tokens\n",
    "    generated_smiles.append(completion)\n",
    "\n",
    "# Convert SMILES to molecules\n",
    "molecules = []\n",
    "valid_smiles = []\n",
    "for smiles in generated_smiles:\n",
    "    mol = get_mol(smiles)\n",
    "    if mol:\n",
    "        molecules.append(mol)\n",
    "        valid_smiles.append(Chem.MolToSmiles(mol))\n",
    "\n",
    "# Display results\n",
    "print(f\"Generated {len(generated_smiles)} molecules, {len(molecules)} are valid.\")\n",
    "print(\"\\nGenerated SMILES:\")\n",
    "for i, smiles in enumerate(valid_smiles):\n",
    "    print(f\"{i+1}. {smiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generated molecules\n",
    "if molecules:\n",
    "    img = Draw.MolsToGridImage(molecules, molsPerRow=3, subImgSize=(300, 300), legends=[f\"Mol {i+1}\" for i in range(len(molecules))])\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Property-Conditioned Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set property conditioning values\n",
    "prop_values = [0.6, 0.75, 0.9]  # QED values (drug-likeness)\n",
    "\n",
    "# Generate molecules for each property value\n",
    "all_molecules = []\n",
    "all_smiles = []\n",
    "all_props = []\n",
    "\n",
    "for prop_value in prop_values:\n",
    "    print(f\"\\nGenerating molecules with {model_type} = {prop_value}\")\n",
    "    \n",
    "    # Tokenize the context\n",
    "    x = torch.tensor([stoi[s] for s in regex.findall(context)], dtype=torch.long)[None,...].repeat(batch_size, 1).to(device)\n",
    "    \n",
    "    # Set property conditioning\n",
    "    p = torch.tensor([[prop_value]]).repeat(batch_size, 1).to(device)\n",
    "    \n",
    "    # Generate molecules\n",
    "    with torch.no_grad():\n",
    "        y = sample(model, x, block_size, temperature=temperature, sample=True, top_k=top_k, prop=p, scaffold=None)\n",
    "    \n",
    "    # Convert generated tokens to SMILES strings\n",
    "    generated_smiles = []\n",
    "    for gen_mol in y:\n",
    "        completion = ''.join([itos[int(i)] for i in gen_mol])\n",
    "        completion = completion.replace('<', '')  # Remove padding tokens\n",
    "        generated_smiles.append(completion)\n",
    "    \n",
    "    # Convert SMILES to molecules\n",
    "    molecules = []\n",
    "    valid_smiles = []\n",
    "    for smiles in generated_smiles:\n",
    "        mol = get_mol(smiles)\n",
    "        if mol:\n",
    "            molecules.append(mol)\n",
    "            valid_smiles.append(Chem.MolToSmiles(mol))\n",
    "    \n",
    "    # Store results\n",
    "    all_molecules.extend(molecules)\n",
    "    all_smiles.extend(valid_smiles)\n",
    "    all_props.extend([prop_value] * len(molecules))\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Generated {len(generated_smiles)} molecules, {len(molecules)} are valid.\")\n",
    "    for i, smiles in enumerate(valid_smiles[:3]):  # Show only first 3\n",
    "        print(f\"{i+1}. {smiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the property-conditioned molecules\n",
    "if all_molecules:\n",
    "    # Show only up to 9 molecules\n",
    "    display_mols = all_molecules[:min(9, len(all_molecules))]\n",
    "    display_props = all_props[:min(9, len(all_molecules))]\n",
    "    \n",
    "    img = Draw.MolsToGridImage(display_mols, molsPerRow=3, subImgSize=(300, 300), \n",
    "                              legends=[f\"{model_type}={p}\" for p in display_props])\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaffold-Conditioned Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaffold conditions\n",
    "scaffolds = [\n",
    "    'c1ccccc1',  # Benzene\n",
    "    'c1ccncc1',  # Pyridine\n",
    "    'c1ccccc1N'  # Aniline\n",
    "]\n",
    "\n",
    "# Generate molecules for each scaffold\n",
    "all_scaffold_molecules = []\n",
    "all_scaffold_smiles = []\n",
    "all_scaffold_conditions = []\n",
    "\n",
    "for scaffold in scaffolds:\n",
    "    print(f\"\\nGenerating molecules with scaffold: {scaffold}\")\n",
    "    \n",
    "    # Tokenize the context\n",
    "    x = torch.tensor([stoi[s] for s in regex.findall(context)], dtype=torch.long)[None,...].repeat(batch_size, 1).to(device)\n",
    "    \n",
    "    # Pad the scaffold\n",
    "    padded_scaffold = scaffold + '<' * (scaffold_max_len - len(regex.findall(scaffold)))\n",
    "    \n",
    "    # Tokenize the scaffold\n",
    "    sca = torch.tensor([stoi[s] for s in regex.findall(padded_scaffold)], dtype=torch.long)[None,...].repeat(batch_size, 1).to(device)\n",
    "    \n",
    "    # Generate molecules\n",
    "    with torch.no_grad():\n",
    "        y = sample(model, x, block_size, temperature=temperature, sample=True, top_k=top_k, prop=None, scaffold=sca)\n",
    "    \n",
    "    # Convert generated tokens to SMILES strings\n",
    "    generated_smiles = []\n",
    "    for gen_mol in y:\n",
    "        completion = ''.join([itos[int(i)] for i in gen_mol])\n",
    "        completion = completion.replace('<', '')  # Remove padding tokens\n",
    "        generated_smiles.append(completion)\n",
    "    \n",
    "    # Convert SMILES to molecules\n",
    "    molecules = []\n",
    "    valid_smiles = []\n",
    "    for smiles in generated_smiles:\n",
    "        mol = get_mol(smiles)\n",
    "        if mol:\n",
    "            molecules.append(mol)\n",
    "            valid_smiles.append(Chem.MolToSmiles(mol))\n",
    "    \n",
    "    # Store results\n",
    "    all_scaffold_molecules.extend(molecules)\n",
    "    all_scaffold_smiles.extend(valid_smiles)\n",
    "    all_scaffold_conditions.extend([scaffold] * len(molecules))\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Generated {len(generated_smiles)} molecules, {len(molecules)} are valid.\")\n",
    "    for i, smiles in enumerate(valid_smiles[:3]):  # Show only first 3\n",
    "        print(f\"{i+1}. {smiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the scaffold-conditioned molecules\n",
    "if all_scaffold_molecules:\n",
    "    # Show only up to 9 molecules\n",
    "    display_mols = all_scaffold_molecules[:min(9, len(all_scaffold_molecules))]\n",
    "    display_scaffolds = all_scaffold_conditions[:min(9, len(all_scaffold_molecules))]\n",
    "    \n",
    "    img = Draw.MolsToGridImage(display_mols, molsPerRow=3, subImgSize=(300, 300), \n",
    "                              legends=[f\"Scaffold: {s}\" for s in display_scaffolds])\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "In this notebook, we've demonstrated how to use MolGPT for molecular generation with different conditioning strategies:\n",
    "\n",
    "1. **Unconditional Generation**: Generate molecules without any constraints\n",
    "2. **Property-Conditioned Generation**: Generate molecules with specific property values (QED, LogP, SAS, TPSA)\n",
    "3. **Scaffold-Conditioned Generation**: Generate molecules containing specific molecular scaffolds\n",
    "\n",
    "The MolGPT model provides a powerful and flexible approach to molecular generation, allowing for precise control over the generated structures through various conditioning mechanisms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (molgpt)",
   "language": "python",
   "name": "molgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
